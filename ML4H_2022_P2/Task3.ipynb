{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce1b3b5",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4738ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "from transformers import TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_metric, Metric, load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm \n",
    "from datasets import Features, Value, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from src.data_loading import load_raw_datasets\n",
    "from src.data_processing import get_raw_x_y, encode_labels\n",
    "from src.constants import PATH_PREPROCESSED_DATA\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41debf",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23171b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train, validation, test = load_raw_datasets()\n",
    "\n",
    "set_dict = {'train' : train, \n",
    "                'validation': validation, \n",
    "                'test': test}\n",
    "\n",
    "\n",
    "for key in set_dict.keys():\n",
    "    x_raw, y_raw = get_raw_x_y(set_dict[key])\n",
    "    y_raw = encode_labels(y_raw)\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = x_raw\n",
    "    df['label'] = y_raw\n",
    "    df.to_csv(PATH_PREPROCESSED_DATA+key+\"_with_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493af652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the new compatible dataset object\n",
    "data_files = {\"train\": PATH_PREPROCESSED_DATA+\"/train_with_labels.csv\", \n",
    "              \"test\": PATH_PREPROCESSED_DATA+\"test_with_labels.csv\", \n",
    "              \"validation\": PATH_PREPROCESSED_DATA+\"validation_with_labels.csv\"}\n",
    "\n",
    "class_names = ['BACKGROUND','METHODS','RESULTS','OBJECTIVE','CONCLUSIONS']\n",
    "abstract_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})\n",
    "\n",
    "dataset = load_dataset('csv', data_files=data_files, features=abstract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d64d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get and apply tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16226f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random smaller subset of the whole dataset\n",
    "# CHANGE HERE THE SUBSAMPLE SIZES FOR THE TRAINING AND EVALUATION DATASET IF YOU LIKE\n",
    "# we tested and reported the following subset sizes for the training set: 1k,10k, 20k\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "large_train_dataset = tokenized_datasets[\"train\"]\n",
    "large_eval_dataset = tokenized_datasets[\"validation\"]\n",
    "large_test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa52cc",
   "metadata": {},
   "source": [
    "# Load pretrained model and run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pretrained model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", num_labels=5)\n",
    "\n",
    "model_original = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c069e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers except the last layer, so we only train the last layer\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ee713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    f1_weighted = load_metric(\"f1\").compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1_macro = load_metric(\"f1\").compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    return {'accuracy':accuracy['accuracy'], 'f1_weighted':f1_weighted['f1'], 'f1_macro':f1_macro['f1']}\n",
    "    \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)   \n",
    "\n",
    "# get trainer with default hyper parameters\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335cca05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fine tune the pretrained model only on the last layer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a808f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the fine tuned model on the test set\n",
    "trainer.predict(large_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d266f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the original model with all layers unfrozen\n",
    "\n",
    "model = model_original\n",
    "\n",
    "# get a new trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c878b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fine tune the pretrained model on all layers\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3612a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test the fine tuned model on the test set\n",
    "trainer.predict(large_test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
